{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1399b66-1ec5-4f89-bd89-6781dd85d6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import sqlite3\n",
    "import logging\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6e0cfa9-cead-4c36-9e92-ed53e0853d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging for ETL monitoring\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "728d2a97-0eb4-4bfa-8223-4f9c3f70da16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract - Collect data from a public API (simulating real-time ingestion)\n",
    "def extract_data():\n",
    "    try:\n",
    "        # Using a public API (e.g., mock logistics API; replace with real API if available)\n",
    "        url = \"https://api.example.com/shipments\"  # Placeholder; using sample JSON for demo\n",
    "        sample_data = {\n",
    "            \"shipments\": [\n",
    "                {\"shipment_id\": \"S001\", \"origin\": \"Delhi\", \"destination\": \"Mumbai\", \"weight_kg\": 100, \"status\": \"In Transit\"},\n",
    "                {\"shipment_id\": \"S002\", \"origin\": \"Bangalore\", \"destination\": \"Chennai\", \"weight_kg\": -50, \"status\": None}\n",
    "            ]\n",
    "        }\n",
    "        df = pd.DataFrame(sample_data[\"shipments\"])\n",
    "        logging.info(\"Data extracted successfully\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Extraction failed: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3098559b-b08d-48f1-9870-36e161338ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Transform - Validate and transform data\n",
    "def transform_data(df):\n",
    "    if df is None:\n",
    "        logging.error(\"No data to transform\")\n",
    "        return None\n",
    "    try:\n",
    "        # Data validation\n",
    "        # Check for null shipment_id\n",
    "        if df['shipment_id'].isnull().any():\n",
    "            logging.warning(\"Null shipment_id found; dropping rows\")\n",
    "            df = df.dropna(subset=['shipment_id'])\n",
    "        \n",
    "        # Check for valid weight_kg (> 0)\n",
    "        invalid_weights = df['weight_kg'] <= 0\n",
    "        if invalid_weights.any():\n",
    "            logging.warning(f\"Invalid weights found in {invalid_weights.sum()} rows; setting to median\")\n",
    "            df.loc[invalid_weights, 'weight_kg'] = df['weight_kg'].median()\n",
    "            \n",
    "        # Transform: Convert weight_kg to weight_lbs\n",
    "        df['weight_lbs'] = df['weight_kg'] * 2.20462\n",
    "\n",
    "        # Fill missing status with \"Pending\"\n",
    "        df['status'] = df['status'].fillna(\"Pending\")\n",
    "\n",
    "        # Standardize destination to uppercase\n",
    "        df['destination'] = df['destination'].str.upper()\n",
    "\n",
    "        # Add timestamp for pipeline tracking\n",
    "        df['processed_at'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        logging.info(\"Data transformed successfully\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Transformation failed: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a78f688-cf3d-4f5a-9165-5ec204ae1e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load - Store data in SQLite database (simulating data warehouse)\n",
    "def load_data(df, db_name=\"logistics.db\", table_name=\"shipments\"):\n",
    "    if df is None:\n",
    "        logging.error(\"No data to load\")\n",
    "        return\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_name)\n",
    "        df.to_sql(table_name, conn, if_exists=\"append\", index=False)\n",
    "        conn.close()\n",
    "        logging.info(f\"Data loaded to {db_name} in table {table_name}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Loading failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d12c974a-533d-412b-a905-c41bc9cd7674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Verify loaded data\n",
    "def verify_data(db_name=\"logistics.db\", table_name=\"shipments\"):\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_name)\n",
    "        loaded_df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n",
    "        conn.close()\n",
    "        logging.info(\"Data verification completed\")\n",
    "        return loaded_df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Verification failed: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6d215f5-d236-4d8b-a251-855a67b517e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 10:16:34,983 - INFO - Starting ETL pipeline\n",
      "2025-08-20 10:16:34,988 - INFO - Data extracted successfully\n",
      "2025-08-20 10:16:34,995 - WARNING - Invalid weights found in 1 rows; setting to median\n",
      "2025-08-20 10:16:34,998 - INFO - Data transformed successfully\n",
      "2025-08-20 10:16:35,003 - INFO - Data loaded to logistics.db in table shipments\n",
      "2025-08-20 10:16:35,005 - INFO - Data verification completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Data:\n",
      "   shipment_id     origin destination  weight_kg      status\n",
      "0        S001      Delhi      Mumbai        100  In Transit\n",
      "1        S002  Bangalore     Chennai        -50        None\n",
      "\n",
      "Transformed Data:\n",
      "   shipment_id     origin destination  weight_kg      status  weight_lbs  \\\n",
      "0        S001      Delhi      MUMBAI        100  In Transit    220.4620   \n",
      "1        S002  Bangalore     CHENNAI         25     Pending     55.1155   \n",
      "\n",
      "          processed_at  \n",
      "0  2025-08-20 10:16:34  \n",
      "1  2025-08-20 10:16:34  \n",
      "\n",
      "Loaded Data from Database:\n",
      "   shipment_id     origin destination  weight_kg      status  weight_lbs  \\\n",
      "0        S001      Delhi      MUMBAI        100  In Transit    220.4620   \n",
      "1        S002  Bangalore     CHENNAI         25     Pending     55.1155   \n",
      "\n",
      "          processed_at  \n",
      "0  2025-08-20 10:16:34  \n",
      "1  2025-08-20 10:16:34  \n"
     ]
    }
   ],
   "source": [
    "# Run ETL pipeline\n",
    "logging.info(\"Starting ETL pipeline\")\n",
    "extracted_df = extract_data()\n",
    "print(\"Extracted Data:\\n\", extracted_df)\n",
    "\n",
    "transformed_df = transform_data(extracted_df)\n",
    "print(\"\\nTransformed Data:\\n\", transformed_df)\n",
    "\n",
    "load_data(transformed_df)\n",
    "verified_df = verify_data()\n",
    "print(\"\\nLoaded Data from Database:\\n\", verified_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fd2a0c-c3d5-40c1-8199-1e1a67fa2c22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
